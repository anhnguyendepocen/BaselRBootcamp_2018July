---
title: "Machine Learning"
subtitle: ""
author: "BaselRBootcamp<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'>@therbootcamp</a>"
date: "July 2018"
output:
  xaringan::moon_reader:
    css: ["default", "my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

layout: true

<div class="my-footer"><span>
<a href="https://therbootcamp.github.io/"><font color="#7E7E7E">BaselRBootcamp, July 2018</font></a>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
&emsp;&emsp;&emsp;&emsp;&emsp;
<a href="https://therbootcamp.github.io/"><font color="#7E7E7E">www.therbootcamp.com</font></a>
</span></div> 



```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
set.seed(100)
past <- tibble(id = 1:5,
               sex = sample(c("m", "f"), size  = 5, replace = TRUE),
               age = round(rnorm(5, mean = 45, sd = 5), 0),
               fam_history = sample(c("Yes", "No"), size = 5, replace = TRUE),
               smoking = sample(c(TRUE, FALSE), size = 5, replace = TRUE),
               disease = sample(c(0, 1), size = 5, replace = TRUE))

present <- tibble(id = 91:95,
                  sex = sample(c("m", "f"), size  = 5, replace = TRUE),
               age = round(rnorm(5, mean = 45, sd = 5), 0),
               fam_history = sample(c("Yes", "No"), size = 5, replace = TRUE),
               smoking = sample(c(TRUE, FALSE), size = 5, replace = TRUE),
               disease = rep("?", 5))

library(ggpubr)
```


---
  
```{r, eval = FALSE, echo = FALSE}
# Code to knit slides
xaringan::inf_mr('_sessions/D2S2_MachineLearning/MachineLearning.Rmd')
```


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# see: https://github.com/yihui/xaringan
# install.packages("xaringan")
# see: 
# https://github.com/yihui/xaringan/wiki
# https://github.com/gnab/remark/wiki/Markdown
options(width=110)
options(digits = 4)
library(baselers)

library(tidyverse)
```


# What is machine learning?

.pull-left6[


### Algorithms autonomously learning from data.

Given data, an algorithm tunes its *parameters* to match the data, understand how it works, and make predictions for what will occur in the future.

```{r, echo = FALSE, out.width = "80%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/mldiagram_A.png")
```

]

.pull-right4[

```{r, echo = FALSE, out.width = "70%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/machinelearningcartoon.png")
```


]

---

# Everyone uses machine learning

.pull-left6[

> ### Machine learning drives our algorithms for demand forecasting, product search ranking, product and deals recommendations, merchandising placements, fraud detection, translations, and much more. ~ Jeff Bezos, Amazon founder

]


.pull-right4[

```{r, echo = FALSE, out.width = "100%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/mlexamples.png")
```

]


---

# What is the basic machine learning process?

```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/MLdiagram.png")
```

---

.pull-left45[

# What is a model?

A *formal* (mathematical) procedure describing the relationships between variables.

Most data have one main *criterion* (aka, *dependent*) variable of interest, and several *features* (aka, *independent* variables)

```{r, echo = FALSE}
knitr::kable(past, format = "markdown")
```


]


.pull-right5[

### Decision Tree

```{r, eval = TRUE, echo = FALSE}
knitr::include_graphics("https://github.com/therbootcamp/therbootcamp.github.io/blob/master/_sessions/_image/decision_tree_example.png?raw=true")
```

### Weighted Additive (Regression)


$$\large{Risk = age \times 0.01 + smoking \times 0.20 + fam\_history \times 0.20}$$

]


---

.pull-left45[

# What is model training?

Model *training* (aka, fitting) is the process of matching a model's *parameters* to a specific dataset.

Q: What are the parameters in the two models on the right?

```{r, echo = FALSE}
knitr::kable(past, format = "markdown")
```

]

.pull-right5[

### Decision Tree

```{r, eval = TRUE, echo = FALSE}
knitr::include_graphics("https://github.com/therbootcamp/therbootcamp.github.io/blob/master/_sessions/_image/decision_tree_example.png?raw=true")
```

### Weighted Additive (Regression)


$$\large{Risk = age \times 0.01 + smoking \times 0.20 + fam\_history \times 0.20}$$



]



---

```{r, echo = FALSE, eval = TRUE}
set.seed(104)

N <- 10

x <- rnorm(n = N, mean = 10, sd = 2)
y_1 <- x * 1.2 + rnorm(n = N, mean = 0, sd = .3)
y_2 <-  x * 0 + rnorm(n = N, mean = 0, sd = 2.5) + 2

y_1_n <- x * 1.2 + rnorm(n = N, mean = 0, sd = .4)
y_2_n <- x * 0 + rnorm(n = N, mean = 0, sd = 2.5) + 2


data <- data.frame(x, y_1, y_2, y_1_n, y_2_n)


plot1 <- ggplot(data = data,
                aes(x = x, y = y_1)) +
  geom_point(size = 5, col = "gray") + 
  geom_point(size = 5, pch = 21) +
  xlim(7, 13) + 
  ylim(7, 15) + 
  labs(title = "Dataset A") + theme_bw()

plot2 <- ggplot(data = data,
                aes(x = x, y = y_2)) +
  geom_point(size = 5, col = "gray") + 
  geom_point(size = 5, pch = 21) +
  labs(title = "Dataset B") + theme_bw() +
  ylim(-2, 5)


plot1b <- plot1 + 
  geom_point(aes(x = x, y = y_1_n), col = "violetred2", size = 5) +
  geom_point(aes(x = x, y = y_1_n), pch = 21, size = 5) +
  geom_abline(slope = 1.2, intercept = 0, size = 2, col = "green") +
  stat_smooth(method="lm",fullrange=TRUE, se = FALSE)
  
plot2b <- plot2 + 
  geom_point(aes(x = x, y = y_2_n), col = "violetred2", size = 5) +
  geom_point(aes(x = x, y = y_2_n), pch = 21, size = 5)+
  geom_abline(slope = 0, intercept = 2, size = 2, col = "green") +
  stat_smooth(method="lm",fullrange=TRUE, se = FALSE)


```



## Fit your own linear model!

<br>
```{r, echo = FALSE, fig.width = 8, fig.height = 4, warning = FALSE, fig.align = 'center', out.width = "85%"}
ggarrange(plot1, plot2)
```


---

## Fit your own linear model!
<br>
```{r, echo = FALSE, fig.width = 8, fig.height = 4, warning = FALSE, fig.align = 'center', out.width = "85%"}
ggarrange(plot1b, plot2)
```


---

## Fit your own linear model!
<br>
```{r, echo = FALSE, fig.width = 8, fig.height = 4, warning = FALSE, fig.align = 'center', out.width = "85%"}
ggarrange(plot1b, plot2b)
```


---

# Why do we separate training from prediction?

.pull-left4[
<br>
Just because a model can match past (training) data well, does *not* necessarily mean that it will *predict* new data well.

Anyone can come up with a model of *past* data (e.g.; stock performance, lottery winnings). 

Predicting what you can't see in the future is much more difficult.

]
 
.pull-right6[

```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/Erfurt_2018June/master/_sessions/_image/prediction_collage.png")
```


]

---
<br><br>
<font size = 6>Can you come up with a model that will perfectly match past data but is worthless in predicting future data?</font><br><br>


.pull-left45[


### Past "Training" Data
<br>
```{r, results = 'asis', echo = FALSE}
knitr::kable(past, format = "markdown")
```

]


.pull-right45[

### Future "Test" Data
<br>

```{r, echo = FALSE}
knitr::kable(present, format = "markdown")

```

]



---

## Two types of prediction tasks

.pull-left45[

```{r, echo = FALSE, out.width = "100%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/classification_task.png")
```


]


.pull-right45[

```{r, echo = FALSE, out.width = "100%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/regression_task.png")
```

]


---

## What machine learning algorithms are there?

.pull-left55[

There are thousands of machine learning algorithms from many different fields.
  - Computer vision, natural language processing, reinforcement learning...

[Wikipedia](https://en.wikipedia.org/wiki/Machine_learning) lists 57 *categories* (!) of machine learning algorithms

```{r, echo = FALSE, eval = TRUE, out.width = "80%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/wikipediaml.png")
```

]

.pull-right4[
<br><br>

### 3 Algorithims

We will focus on 3 algorithms that apply to most ML tasks:

| Algorithm|Complexity|
|:------|:----|
|     [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree)| Low |
|     [Regression](https://en.wikipedia.org/wiki/Regression_analysis)| Low / Medium | 
|     [Random Forests](https://en.wikipedia.org/wiki/Random_forest)| High |

]

---

## How do you fit and evaluate ML models in R?

.pull-left45[

Answer: Pretty much the same way you fit standard statistical models. Install the package, load, and find the main fitting functions.

```{r, eval = FALSE}
# Install the glmnet package
install.packages("glmnet")

# Load glmnet
library(glmnet)

# Look at help menu
?glmnet
```

<b>Important!</b> Look at the help file for each function!

Some functions will use the standard `FUN(formula, data)` arguments, but others (like `glmnet()`) require other arguments, like `x, y` (numeric matrices).

]

.pull-right5[

```{r, eval = FALSE}
# Help file for glmnet
?glmnet
```


```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/Erfurt_2018June/master/_sessions/_image/glmnet_help.jpg")
```

]

---

## Regression

.pull-left45[

In regression, the criterion is modeled as the weighted sum of predictors times *weights* $\beta_{1}$, $\beta_{2}$

### Loan Default:

One could model the risk of defaulting on a loan as:

$$Risk = Age \times \beta_{age} + Income \times \beta_{income} + ...$$

Training a model means finding values of $\beta_{Age}$ and $\beta_{Income}$ that 'best' match the training data.

```{r, echo = FALSE, eval = TRUE, out.width = "50%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/regression.png")
```

]

.pull-right5[
<br><br>
### Regression with glm()

The `glm()` function in the base stats package performs standard regression

```{r, eval = FALSE}
# Standard linear regression
glm_mod <- glm(formula = happiness ~ .,
               data = baselers)

# Logisitic regression with family = 'binomial'
glm_mod <- glm(formula = sex ~ .,
               data = baselers.
               family = "binomial")
```



]

---

## Decision Trees

.pull-left45[

In decision trees, the criterion is modeled as a sequence of logical Yes or No questions.

### Loan Default:

```{r, echo = FALSE, eval = TRUE}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/defaulttree.png")
```


]

.pull-right5[
<br><br>
### Decision trees with rpart

Create decision trees with `rpart`

```{r, eval = FALSE}
install.packages("rpart")
library(rpart)

# Train rpart model
loan_rpart_mod <- rpart(formula, data,
                        method = "class",
                        rpart.control)
```

]

---

## Random Forest

.pull-left45[

A [Random Forest](https://en.wikipedia.org/wiki/Random_forest) is a collection of many (hundreds, thousands) of decision trees that use different features

```{r, echo = FALSE, eval = TRUE, out.width = "90%", fig.cap = "<font size=3><a href='https://medium.com/@williamkoehrsen'>Sourcemedium.com</a></font>"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/Erfurt_2018June/master/_sessions/_image/randomforest_diagram.png")
```
 
]

.pull-right5[

### Random Forest trees with randomforest

```{r, eval = FALSE}
install.packages("randomForest")
library(randomForest)

# Create a randomForest model
randomForest(formula = y ~.,    # Formula 
             data = data_train, # Training data
             ntree, mtry)  # Tuning parameters
```

Tuning parameters

|Parameter | Description|
|:-------|:-------|
|`ntree`|Number of trees in forest|
|`mtry`|Number of variables randomly selected at splits|

]


---
.pull-left35[

## Exploring machine learning objects

Just like objects from statistical functions, objects from machine learning functions are lists that you can explore using *generic functions*:

|Function|Description
|:------|:----|
|`summary()`| Overview of the most important information|
|`names()`|See all named elements you can access with $|
|`plot()`|Visualise the object (sometimes)|

]

.pull-right6[


```{r, eval = FALSE, echo = TRUE}
# Create a regression object
baselers_glm <- glm(income ~ age + height + children,
                    data = baselers)

# Look at summary results
summary(baselers_glm)
# [...]
```

```{r, eval = TRUE, echo = FALSE}
baselers_glm <- glm(income ~ age + height + children,
                    data = baselers)
```


```{r}
# Look at all named outputs
names(baselers_glm)

# Access specific outputs
baselers_glm$coefficients
```


]


---

.pull-left4[

# Predict new data with predict()

All machine learning objects will allow you to predict the criterion of new data using `predict()`

|argument|description|
|:----|:-----|
|object| A machine learning / statistical object created from `glm()`, `randomforest()`, ...|
|newdata|A dataframe of new data|


```{r, echo = FALSE}
zurichers <- tibble(id = c(1, 2, 3, 4, 5),
                    age = c(65, 75, 35, 54, 65),
                    children = c(0, 3, 1, 0, 2),
                    height = c(1.66, 1.96, 1.76, 1.73, 1.59),
                    income = c(7500, 5400, 8400, 9500, 3700))
```



]

.pull-right55[


zurichers dataframe:

```{r, echo = FALSE}
knitr::kable(zurichers, format = "markdown")
```


```{r}
predict(object = baselers_glm,  # ML object
        newdata = zurichers)    # DF of new data
```

The output is a vector of predicted values of the new dataset! You can now compare these to the true criterion values of `newdata` to see how well your model did.

]


---
<br>

## Questions?

## [Demo and Practical](https://therbootcamp.github.io/BaselRBootcamp_2018July/_sessions/MachineLearning/MachineLearning_practical.html)

```{r, echo = FALSE, out.width = "70%", fig.align="center"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/MLdiagram.png")
```
