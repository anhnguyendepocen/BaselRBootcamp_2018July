<!DOCTYPE html>
<html>
  <head>
    <title>Machine Learning</title>
    <meta charset="utf-8">
    <meta name="author" content="BaselRBootcamp www.therbootcamp.com @therbootcamp" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Machine Learning
### BaselRBootcamp<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'><span class="citation">@therbootcamp</span></a>
### July 2018

---


layout: true

&lt;div class="my-footer"&gt;&lt;span&gt;
&lt;a href="https://therbootcamp.github.io/"&gt;&lt;font color="#7E7E7E"&gt;BaselRBootcamp, July 2018&lt;/font&gt;&lt;/a&gt;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&lt;a href="https://therbootcamp.github.io/"&gt;&lt;font color="#7E7E7E"&gt;www.therbootcamp.com&lt;/font&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/div&gt; 






---
  






# What is machine learning?

.pull-left6[


### Algorithms autonomously learning from data.

Given data, an algorithm tunes its *parameters* to match the data, understand how it works, and make predictions for what will occur in the future.

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/mldiagram_A.png" width="80%" style="display: block; margin: auto;" /&gt;

]

.pull-right4[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/machinelearningcartoon.png" width="70%" style="display: block; margin: auto;" /&gt;


]

---

# Everyone uses machine learning

.pull-left6[

&gt; ### Machine learning drives our algorithms for demand forecasting, product search ranking, product and deals recommendations, merchandising placements, fraud detection, translations, and much more. ~ Jeff Bezos, Amazon founder

]


.pull-right4[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/mlexamples.png" width="100%" style="display: block; margin: auto;" /&gt;

]


---

# What is the basic machine learning process?

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/MLdiagram.png" width="90%" /&gt;

---

.pull-left45[

# What is a model?

A *formal* (mathematical) procedure describing the relationships between variables.

Most data have one main *criterion* (aka, *dependent*) variable of interest, and several *features* (aka, *independent* variables)


| id|sex | age|fam_history |smoking | disease|
|--:|:---|---:|:-----------|:-------|-------:|
|  1|m   |  45|No          |FALSE   |       0|
|  2|m   |  43|Yes         |FALSE   |       1|
|  3|f   |  40|Yes         |FALSE   |       1|
|  4|m   |  51|Yes         |FALSE   |       1|
|  5|m   |  44|No          |TRUE    |       0|


]


.pull-right5[

### Decision Tree

![](https://github.com/therbootcamp/therbootcamp.github.io/blob/master/_sessions/_image/decision_tree_example.png?raw=true)&lt;!-- --&gt;

### Weighted Additive (Regression)


`$$\large{Risk = age \times 0.01 + smoking \times 0.20 + fam\_history \times 0.20}$$`

]


---

.pull-left45[

# What is model training?

Model *training* (aka, fitting) is the process of matching a model's *parameters* to a specific dataset.

Q: What are the parameters in the two models on the right?


| id|sex | age|fam_history |smoking | disease|
|--:|:---|---:|:-----------|:-------|-------:|
|  1|m   |  45|No          |FALSE   |       0|
|  2|m   |  43|Yes         |FALSE   |       1|
|  3|f   |  40|Yes         |FALSE   |       1|
|  4|m   |  51|Yes         |FALSE   |       1|
|  5|m   |  44|No          |TRUE    |       0|

]

.pull-right5[

### Decision Tree

![](https://github.com/therbootcamp/therbootcamp.github.io/blob/master/_sessions/_image/decision_tree_example.png?raw=true)&lt;!-- --&gt;

### Weighted Additive (Regression)


`$$\large{Risk = age \times 0.01 + smoking \times 0.20 + fam\_history \times 0.20}$$`



]



---





## Fit your own linear model!

&lt;br&gt;
&lt;img src="MachineLearning_files/figure-html/unnamed-chunk-12-1.png" width="85%" style="display: block; margin: auto;" /&gt;


---

## Fit your own linear model!
&lt;br&gt;
&lt;img src="MachineLearning_files/figure-html/unnamed-chunk-13-1.png" width="85%" style="display: block; margin: auto;" /&gt;


---

## Fit your own linear model!
&lt;br&gt;
&lt;img src="MachineLearning_files/figure-html/unnamed-chunk-14-1.png" width="85%" style="display: block; margin: auto;" /&gt;


---

# Why do we separate training from prediction?

.pull-left4[
&lt;br&gt;
Just because a model can match past (training) data well, does *not* necessarily mean that it will *predict* new data well.

Anyone can come up with a model of *past* data (e.g.; stock performance, lottery winnings). 

Predicting what you can't see in the future is much more difficult.

]
 
.pull-right6[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/Erfurt_2018June/master/_sessions/_image/prediction_collage.png" width="80%" /&gt;


]

---
&lt;br&gt;&lt;br&gt;
&lt;font size = 6&gt;Can you come up with a model that will perfectly match past data but is worthless in predicting future data?&lt;/font&gt;&lt;br&gt;&lt;br&gt;


.pull-left45[


### Past "Training" Data
&lt;br&gt;

| id|sex | age|fam_history |smoking | disease|
|--:|:---|---:|:-----------|:-------|-------:|
|  1|m   |  45|No          |FALSE   |       0|
|  2|m   |  43|Yes         |FALSE   |       1|
|  3|f   |  40|Yes         |FALSE   |       1|
|  4|m   |  51|Yes         |FALSE   |       1|
|  5|m   |  44|No          |TRUE    |       0|

]


.pull-right45[

### Future "Test" Data
&lt;br&gt;


| id|sex | age|fam_history |smoking |disease |
|--:|:---|---:|:-----------|:-------|:-------|
| 91|m   |  51|Yes         |TRUE    |?       |
| 92|f   |  47|No          |TRUE    |?       |
| 93|m   |  39|No          |TRUE    |?       |
| 94|f   |  51|Yes         |TRUE    |?       |
| 95|f   |  50|Yes         |FALSE   |?       |

]



---

## Two types of prediction tasks

.pull-left45[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/classification_task.png" width="100%" style="display: block; margin: auto;" /&gt;


]


.pull-right45[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/regression_task.png" width="100%" style="display: block; margin: auto;" /&gt;

]


---

## What machine learning algorithms are there?

.pull-left55[

There are thousands of machine learning algorithms from many different fields.
  - Computer vision, natural language processing, reinforcement learning...

[Wikipedia](https://en.wikipedia.org/wiki/Machine_learning) lists 57 *categories* (!) of machine learning algorithms

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/wikipediaml.png" width="80%" style="display: block; margin: auto;" /&gt;

]

.pull-right4[
&lt;br&gt;&lt;br&gt;

### 3 Algorithims

We will focus on 3 algorithms that apply to most ML tasks:

| Algorithm|Complexity|
|:------|:----|
|     [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree)| Low |
|     [Regression](https://en.wikipedia.org/wiki/Regression_analysis)| Low / Medium | 
|     [Random Forests](https://en.wikipedia.org/wiki/Random_forest)| High |

]

---

## How do you fit and evaluate ML models in R?

.pull-left45[

Answer: Pretty much the same way you fit standard statistical models. Install the package, load, and find the main fitting functions.


```r
# Install the glmnet package
install.packages("glmnet")

# Load glmnet
library(glmnet)

# Look at help menu
?glmnet
```

&lt;b&gt;Important!&lt;/b&gt; Look at the help file for each function!

Some functions will use the standard `FUN(formula, data)` arguments, but others (like `glmnet()`) require other arguments, like `x, y` (numeric matrices).

]

.pull-right5[


```r
# Help file for glmnet
?glmnet
```


&lt;img src="https://raw.githubusercontent.com/therbootcamp/Erfurt_2018June/master/_sessions/_image/glmnet_help.jpg" width="80%" /&gt;

]

---

## Regression

.pull-left45[

In regression, the criterion is modeled as the weighted sum of predictors times *weights* `\(\beta_{1}\)`, `\(\beta_{2}\)`

### Loan Default:

One could model the risk of defaulting on a loan as:

`$$Risk = Age \times \beta_{age} + Income \times \beta_{income} + ...$$`

Training a model means finding values of `\(\beta_{Age}\)` and `\(\beta_{Income}\)` that 'best' match the training data.

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/regression.png" width="50%" style="display: block; margin: auto;" /&gt;

]

.pull-right5[
&lt;br&gt;&lt;br&gt;
### Regression with glm()

The `glm()` function in the base stats package performs standard regression


```r
# Standard linear regression
glm_mod &lt;- glm(formula = happiness ~ .,
               data = baselers)

# Logisitic regression with family = 'binomial'
glm_mod &lt;- glm(formula = sex ~ .,
               data = baselers.
               family = "binomial")
```



]

---

## Decision Trees

.pull-left45[

In decision trees, the criterion is modeled as a sequence of logical Yes or No questions.

### Loan Default:

![](https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/defaulttree.png)&lt;!-- --&gt;


]

.pull-right5[
&lt;br&gt;&lt;br&gt;
### Decision trees with rpart

Create decision trees with `rpart`


```r
install.packages("rpart")
library(rpart)

# Train rpart model
loan_rpart_mod &lt;- rpart(formula, data,
                        method = "class",
                        rpart.control)
```

]

---

## Random Forest

.pull-left45[

A [Random Forest](https://en.wikipedia.org/wiki/Random_forest) is a collection of many (hundreds, thousands) of decision trees that use different features

&lt;div class="figure"&gt;
&lt;img src="https://raw.githubusercontent.com/therbootcamp/Erfurt_2018June/master/_sessions/_image/randomforest_diagram.png" alt="&amp;lt;font size=3&amp;gt;&amp;lt;a href='https://medium.com/@williamkoehrsen'&amp;gt;Sourcemedium.com&amp;lt;/a&amp;gt;&amp;lt;/font&amp;gt;" width="90%" /&gt;
&lt;p class="caption"&gt;&lt;font size=3&gt;&lt;a href='https://medium.com/@williamkoehrsen'&gt;Sourcemedium.com&lt;/a&gt;&lt;/font&gt;&lt;/p&gt;
&lt;/div&gt;
 
]

.pull-right5[

### Random Forest trees with randomforest


```r
install.packages("randomForest")
library(randomForest)

# Create a randomForest model
randomForest(formula = y ~.,    # Formula 
             data = data_train, # Training data
             ntree, mtry)  # Tuning parameters
```

Tuning parameters

|Parameter | Description|
|:-------|:-------|
|`ntree`|Number of trees in forest|
|`mtry`|Number of variables randomly selected at splits|

]


---
.pull-left35[

## Exploring machine learning objects

Just like objects from statistical functions, objects from machine learning functions are lists that you can explore using *generic functions*:

|Function|Description
|:------|:----|
|`summary()`| Overview of the most important information|
|`names()`|See all named elements you can access with $|
|`plot()`|Visualise the object (sometimes)|

]

.pull-right6[



```r
# Create a regression object
baselers_glm &lt;- glm(income ~ age + height + children,
                    data = baselers)

# Look at summary results
summary(baselers_glm)
# [...]
```





```r
# Look at all named outputs
names(baselers_glm)
```

```
##  [1] "coefficients"      "residuals"         "fitted.values"     "effects"           "R"                
##  [6] "rank"              "qr"                "family"            "linear.predictors" "deviance"         
## [11] "aic"               "null.deviance"     "iter"              "weights"           "prior.weights"    
## [16] "df.residual"       "df.null"           "y"                 "converged"         "boundary"         
## [21] "model"             "na.action"         "call"              "formula"           "terms"            
## [26] "data"              "offset"            "control"           "method"            "contrasts"        
## [31] "xlevels"
```

```r
# Access specific outputs
baselers_glm$coefficients
```

```
## (Intercept)         age      height    children 
##     574.740     149.302       1.720       7.727
```


]


---

.pull-left4[

# Predict new data with predict()

All machine learning objects will allow you to predict the criterion of new data using `predict()`

|argument|description|
|:----|:-----|
|object| A machine learning / statistical object created from `glm()`, `randomforest()`, ...|
|newdata|A dataframe of new data|






]

.pull-right55[


zurichers dataframe:


| id| age| children| height| income|
|--:|---:|--------:|------:|------:|
|  1|  65|        0|   1.66|   7500|
|  2|  75|        3|   1.96|   5400|
|  3|  35|        1|   1.76|   8400|
|  4|  54|        0|   1.73|   9500|
|  5|  65|        2|   1.59|   3700|



```r
predict(object = baselers_glm,  # ML object
        newdata = zurichers)    # DF of new data
```

```
##     1     2     3     4     5 
## 10282 11799  5811  8640 10298
```

The output is a vector of predicted values of the new dataset! You can now compare these to the true criterion values of `newdata` to see how well your model did.

]


---
&lt;br&gt;

## Questions?

## [Demo and Practical](https://therbootcamp.github.io/BaselRBootcamp_2018July/_sessions/MachineLearning/MachineLearning_practical.html)

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/MLdiagram.png" width="70%" style="display: block; margin: auto;" /&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
