<!DOCTYPE html>
<html>
  <head>
    <title>Machine Learning II</title>
    <meta charset="utf-8">
    <meta name="author" content="Basel R Bootcamp www.therbootcamp.com @therbootcamp" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="baselrbootcamp.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Machine Learning II
## caret
### Basel R Bootcamp<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'><span class="citation">@therbootcamp</span></a>
### July 2018

---


layout: true

&lt;div class="my-footer"&gt;&lt;span&gt;
&lt;a href="https://therbootcamp.github.io/"&gt;&lt;font color="#7E7E7E"&gt;BaselRBootcamp, July 2018&lt;/font&gt;&lt;/a&gt;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&lt;a href="https://therbootcamp.github.io/"&gt;&lt;font color="#7E7E7E"&gt;www.therbootcamp.com&lt;/font&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/div&gt; 

---



&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
# Recap from Machine Learning

---


# What is machine learning?

.pull-left6[


### Algorithms autonomously learning from data.

Given data, an algorithm tunes its *parameters* to match the data, understand how it works, and make predictions for what will occur in the future.

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/mldiagram_A.png" width="80%" style="display: block; margin: auto;" /&gt;

]

.pull-right4[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/machinelearningcartoon.png" width="70%" style="display: block; margin: auto;" /&gt;


]
---

# What is the basic machine learning process?

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/MLdiagram.png" width="90%" /&gt;



---

# Why do we separate training from prediction?

.pull-left35[
&lt;br&gt;
Just because a model can &lt;high&gt;fit past data well&lt;/high&gt;, does *not* necessarily mean that it will &lt;high&gt;predict new data well&lt;/high&gt;.

Anyone can come up with a model of past data (e.g.; stock performance, lottery winnings). 

&lt;high&gt;Predicting what you can't see in the future is much more difficult.&lt;/high&gt;

&gt; "An economist is an expert who will know tomorrow why the things he predicted yesterday didn't happen today." ~ Evan Esar


]
 
.pull-right6[

&lt;p align="center"&gt;
  &lt;img src="https://raw.githubusercontent.com/therbootcamp/Erfurt_2018June/master/_sessions/_image/prediction_collage.png"&gt;
&lt;/p&gt;


]



---
&lt;br&gt;&lt;br&gt;
&lt;font size = 6&gt;Can you come up with a model that will perfectly match past data but is worthless in predicting future data?&lt;/font&gt;&lt;br&gt;&lt;br&gt;




.pull-left45[


### Past "Training" Data
&lt;br&gt;

| id|sex | age|fam_history |smoking | disease|
|--:|:---|---:|:-----------|:-------|-------:|
|  1|m   |  45|No          |FALSE   |       0|
|  2|m   |  43|Yes         |FALSE   |       1|
|  3|f   |  40|Yes         |FALSE   |       1|
|  4|m   |  51|Yes         |FALSE   |       1|
|  5|m   |  44|No          |TRUE    |       0|

]


.pull-right45[

### Future "Test" Data
&lt;br&gt;


| id|sex | age|fam_history |smoking |disease |
|--:|:---|---:|:-----------|:-------|:-------|
| 91|m   |  51|Yes         |TRUE    |?       |
| 92|f   |  47|No          |TRUE    |?       |
| 93|m   |  39|No          |TRUE    |?       |
| 94|f   |  51|Yes         |TRUE    |?       |
| 95|f   |  50|Yes         |FALSE   |?       |

]

---

## Two types of prediction tasks

.pull-left45[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/classification_task.png" width="100%" style="display: block; margin: auto;" /&gt;

]

.pull-right45[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/regression_task.png" width="100%" style="display: block; margin: auto;" /&gt;

]

---

## Overfitting

&lt;img src="MachineLearningII_files/figure-html/unnamed-chunk-9-1.png" width="100%" /&gt;






---

## Optimizing model parameters with cross validation

.pull-left45[

Most ML models have two types of parameters, &lt;high&gt;raw parameters&lt;/high&gt; that dictate how an individual model makes predictions, and &lt;high&gt;tuning parameters&lt;/high&gt; that determine how those raw parameters are calculated.

|Model| Raw parameters|Tuning parameters|
|:------|:------|:------|
|Decision Trees|Nodes, splits, decisions |Minimum number of cases in each node|
|Regularised regression |Model coefficients | Coefficient penalty term|

To determine 'optimal' tuning parameters, which maximize prediction performance, techniques such as &lt;high&gt;cross validation&lt;/high&gt; are often used.

]

.pull-right5[

### Cross Validation

&lt;img src="https://raw.githubusercontent.com/therbootcamp/Erfurt_2018June/master/_sessions/_image/crossvalidation.jpg" width="80%" style="display: block; margin: auto;" /&gt;

0) Select *tuning parameters*

1) Split the training set into K "folds"

2) Use K - 1 folds for training, and 1 fold for testing.

3) Repeat K times.

4) Average the K testing performances

]


---

.pull-left55[

# Caret



Caret stands for &lt;b&gt;C&lt;/b&gt;lassification &lt;b&gt;A&lt;/b&gt;nd &lt;b&gt;RE&lt;/b&gt;gression &lt;b&gt;T&lt;/b&gt;raining.

`caret` is a 'wrapper' packages that automates much of the the machine learning process.

Do very complex machine learning tasks with a few simple functions

Use any of hundreds of different ML algorithms by changing one "string' (not line!) to use a different model


```r
library(caret)

train(..., method = "lm") # Regression!
train(..., method = "rf") # Random forests!
train(..., method = "ada") # Boosted trees
```

Knows each model's tuning parameters and chooses the best ones for your data using cross validation (or other method).

]

.pull-right4[
&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/09/Caret-package-in-R.png" alt="The almighty Caret!" width="90%" /&gt;
&lt;p class="caption"&gt;The almighty Caret!&lt;/p&gt;
&lt;/div&gt;

&lt;img src="https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg" width="100%" style="display: block; margin: auto;" /&gt;


]


---
.pull-left5[

# Caret




As always, you can install `caret` from CRAN


```r
# Install caret
install.packages("caret")

# Load the caret package
library("caret")
```

Once you've installed `caret`, look at the vignette for a nice overview of the package


```r
# Open the main package vignette
vignette("caret")
```

Today we will go over the main functions in the package

]

.pull-right45[

### Caret Vignettes

The `caret` package has some of the *best* documentation (vignettes) you'll ever see.

&lt;img src="caret_vignette.jpg" width="90%" /&gt;

]

---

.pull-left5[
# Caret

Here are the main functions we will cover in the `caret` package

| Function| Purpose|
|--------|----------|
| `createDataPartition()` | Split data into different partitions|
| `trainControl()` | Determine how training (in general) will be done|
| `train()` | Specify a model and find *best* parameters|
| `varImp()` | Determine variable importance |
| `predict()` | Predict values for new data|
| `postResample()` | Evaluate prediction performance|

]

.pull-right45[

### Caret Vignettes

The `caret` package has some of the *best* documentation (vignettes) you'll ever see.

&lt;img src="caret_vignette.jpg" width="90%" /&gt;

]


---

.pull-left6[
### createDataPartition()

Use `createDataPartition()` to &lt;high&gt;split a dataset&lt;/high&gt; into separate partitions.

Useful to split a large dataset into separate training and test data


```r
data &lt;- baselers %&gt;% 
  drop_na     # Drop missing cases from baselers

# Get training cases
train_v &lt;- createDataPartition(y = data$income, #crit
                               times = 1, 
                               p = .5)$Resample1

# Vector of training cases
train_v[1:10]
```

```
##  [1]  3  4  6  8 10 11 12 15 17 18
```

```r
# Training Data
baselers_train &lt;- data %&gt;% 
  slice(train_v)

# Testing Data
baselers_test &lt;- data %&gt;% 
  slice(-train_v)
```


]

.pull-right35[

### Training and Test


```r
### Training data

dim(baselers_train)
```

```
## [1] 3072   20
```

```r
baselers_train %&gt;%
  slice(1:5) %&gt;%
  select(1:3)
```

```
## # A tibble: 5 x 3
##      id sex      age
##   &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;
## 1     4 male      27
## 2     6 male      63
## 3     8 female    41
## 4    10 female    31
## 5    12 male      31
```

```r
### Testing data

dim(baselers_test)
```

```
## [1] 3068   20
```


]


---
.pull-left55[
### trainControl()

Use `trainControl()` to define how `carat` should select the best parameters for any ML model

Here you can specify repeated &lt;high&gt;cross validation&lt;/high&gt;

Many other methods are available in the `method` argument, check the help menu with `?trainControl`


```r
# Define how caret should fit models

ctrl &lt;- trainControl(method = "repeatedcv",
                     number = 10,  # 10 folds
                     repeats = 2) # Repeat 2 times
```

### trainControl methods

|method|Description|
|:----|:----|
|`"repeatedcv"` | Repeated cross-validation|
|`"LOOCV"`| Leave one out cross-validation|
|`"none"` | Fit one model with default parameters |

]

.pull-right4[
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
### Cross Validation
![](https://raw.githubusercontent.com/therbootcamp/Erfurt_2018June/master/_sessions/_image/crossvalidation.jpg)&lt;!-- --&gt;

]

---

.pull-left55[

### train()

Use `train()` to &lt;high&gt;fit any&lt;/high&gt; of over 280 models using &lt;high&gt;optimal parameters&lt;/high&gt;


```r
rpart_train &lt;- train(form = income ~ .,  
                     data = baselers_train,
                     method = "rpart", # Model!
                     trControl = ctrl)
```


|Parameter|Description|
|:-----|:----|
|`form`|Formula specifying criterion|
|`data`|Training data|
|`method`| Model|
|`trControl`| Control parameters|


]

.pull-right4[

See all &gt;280 models on the [Caret Documentation Page](http://topepo.github.io/caret/available-models.html)

&lt;img src="caret_models.jpg" width="1699" /&gt;

]


---

### train()

.pull-left55[

For &lt;high&gt;classification tasks&lt;/high&gt;, make sure your &lt;high&gt;criterion is a factor&lt;/high&gt;. For regression tasks, make sure it is numeric




```r
# Diagnosis is currently coded as 0, 1
heartdisease$diagnosis[1:10]

# Will be a regression model
reg_mod &lt;- train(form = diagnosis ~ .,
                 data = heartdisease,
                 method = "rpart")
```

&lt;font color='red' size = 3&gt;Warning messages:...Are you sure you wanted to do regression?&lt;/font&gt;

Use `factor()` to convert your criterion to a factor, now you are doing classification!


```r
# Use factor() to specify a classification model
class_mod &lt;- train(form = factor(diagnosis) ~ .,
                   data = heartdisease,
                   method = "rpart")
```


]

.pull-right4[

[Caret Documentation Page](http://topepo.github.io/caret/available-models.html)

&lt;img src="caret_models.jpg" width="1699" /&gt;

]


---

### train()

Print the result of `train()` to obtain overall results


```r
rpart_train
```

```
## CART 
## 
## 3072 samples
##   19 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 2 times) 
## Summary of sample sizes: 2764, 2764, 2765, 2766, 2765, 2765, ... 
## Resampling results across tuning parameters:
## 
##   cp       RMSE  Rsquared  MAE 
##   0.06978  1461  0.7113    1177
##   0.10853  1814  0.5549    1447
##   0.54480  2211  0.5241    1782
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was cp = 0.06978.
```



---
.pull-left4[

### train()

Explore your `train` Object


```r
# Best tuning parameters
rpart_train$bestTune
```

```
##        cp
## 1 0.06978
```

```r
# Show the final model
rpart_train$finalModel
```

```
## n= 3072 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 3072 2.271e+10  7522  
##   2) age&lt; 46.5 1826 5.051e+09  5864 *
##   3) age&gt;=46.5 1246 5.289e+09  9952  
##     6) age&lt; 66.5 919 1.975e+09  9113 *
##     7) age&gt;=66.5 327 8.484e+08 12310 *
```

]


.pull-right55[


```r
# Plot final model
plot(rpart_train$finalModel)
text(rpart_train$finalModel)
```

&lt;img src="MachineLearningII_files/figure-html/unnamed-chunk-32-1.png" style="display: block; margin: auto;" /&gt;


]

---

.pull-left5[

### varImp()

Use `varImp()` to extract &lt;high&gt;variable importance&lt;/high&gt; from a model

Result will indicate how (relatively) important each variable was in predicting the criterion.
    

```r
# Get veriable importance from rpart_train
varImp(rpart_train)
```

```
## rpart variable importance
## 
##   only 20 most important variables shown (out of 24)
## 
##                                  Overall
## age                              100.000
## food                              57.957
## datause                            1.256
## id                                 0.728
## fitness                            0.626
## consultations                      0.284
## tattoos                            0.283
## fasnachtyes                        0.000
## children                           0.000
## rhine                              0.000
## hiking                             0.000
## eyecoryes                          0.000
## confessionother                    0.000
## weight                             0.000
## educationobligatory_school         0.000
## happiness                          0.000
## educationSEK_II                    0.000
## height                             0.000
## `confessionevangelical-reformed`   0.000
## confessionmuslim                   0.000
```
    
]

.pull-right45[

&lt;br&gt;&lt;br&gt;&lt;br&gt;


```r
# Plot variable importance
plot(varImp(rpart_train))
```

![](MachineLearningII_files/figure-html/unnamed-chunk-34-1.png)&lt;!-- --&gt;

]

---

.pull-left5[
## predict(), postResample()

Test a models prediction performance with &lt;high&gt;predict()&lt;/high&gt;


```r
# Get predictions based on best 
bas_pred &lt;- predict(object = rpart_train, 
                    newdata = baselers_test)

# Result is a vector of predictions 
#   for each row in newdata
bas_pred[1:5]
```

```
##     1     2     3     4     5 
##  5864  9113 12310  5864  5864
```

Then summarise overall performance with &lt;high&gt;postResample()&lt;/high&gt;


```r
# Assess performance with postResample()
postResample(pred = bas_pred, 
             obs = baselers_test$income)
```

```
##      RMSE  Rsquared       MAE 
## 1560.3574    0.6752 1261.1642
```


]

.pull-right45[

### Plot predictions vs Truth


```r
# Plot predictions versus truth
end &lt;- tibble(pred = bas_pred,
              obs = baselers_test$income)

ggplot(data = end,
       aes(x = pred, y = obs)) + 
  geom_point(alpha = .1) +
  labs(title = "Rpart Performance",
       x = "Predictions",
       y = 'Truth') +
  theme_bw() +
  geom_smooth(method = "lm")
```

&lt;img src="MachineLearningII_files/figure-html/unnamed-chunk-37-1.png" style="display: block; margin: auto;" /&gt;


]

---
.pull-left5[

## Recap - ML steps with caret

Step 0: Create training and test data (if necessary)


```r
train_v &lt;- createDataPartition(y, times, p)

data_train &lt;- data %&gt;% slice(train_v)
data_test &lt;- data %&gt;% slice(-train_v)
```

Step 1: Define control parameters


```r
ctl &lt;- trainControl(method = "repeatedcv",
                    number = 10, 
                    repeats = 2) 
```


Step 2: Train model


```r
rpart_train &lt;- train(form = income ~ .,  
                     data = data_train,
                     method = "rpart",
                     trControl = ctl)
```

]

.pull-right45[
&lt;br&gt;&lt;br&gt;&lt;br&gt;
Step 3: Explore


```r
rpart_train            # Print object
varImp(rpart_train)    # Var importance
rpart_train$finalModel # Final model
```


Step 4: Predict 


```r
my_pred &lt;- predict(object = rpart_train, 
                   newdata = data_test)
```


Step 4: Evaluate 


```r
postResample(pred = bas_pred, 
             obs = baselers_test$income)
```


]


---

&lt;br&gt;&lt;br&gt;
.pull-left5[

# Caret

There are *many* other great features of caret we haven't touched

- Use the `preProcess()` function to pre-process your data (Imputing missing values)

- Add your own custom model

Be sure to check out the **excellent** documentation site to learn all the details

]

.pull-right45[

http://topepo.github.io/caret/index.html

&lt;img src="caret_package_md.jpg" width="2251" /&gt;

]

---

.pull-left4[
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
# Questions?

## [Machine Learning II Practical](https://therbootcamp.github.io/BaselRBootcamp_2018July/_sessions/MachineLearningII/MachineLearningII_practical.html)


]

.pull-right55[
&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;div class="figure"&gt;
&lt;img src="https://www.petsmagazine.com.sg/img/articles/main/bugsbunny.jpg" alt="&amp;lt;font size = 4&amp;gt;Source: www.petsmagazine.com&amp;lt;/font&amp;gt;" width="90%" /&gt;
&lt;p class="caption"&gt;&lt;font size = 4&gt;Source: www.petsmagazine.com&lt;/font&gt;&lt;/p&gt;
&lt;/div&gt;

]
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
